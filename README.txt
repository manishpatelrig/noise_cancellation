*******************************************************************************
The file has following sections :

1. Library Dependency
2. Solution Approach
3. Zip File Description
4. Tech Stack Used
5. How to Run


********************************* Library Dependency ************************
The program has following library dependencies :

	1. Numpy
	2. pyAudioAnalysis
	3. logmmse 
	4. eyed3
	5. pydub
	6. hmmlearn
	7. librosa
	8. logmmse
	9. scipy
	10. soundfile
	11. matplotlib
	12. tqdm

Note - all are pip3 downloadable
IInstalling library packages : pip3 install <library_package>

****************************** Solution Approach ******************************

* Approach :

Our approach is based on the principle that the Primary Speaker's audio will be louder than the surrounding and so amplitude must be larger than the rest of the speaker.
We take advantage of this fact to differentiate the Primary Speaker.

The following major tasks has been considered in the program :
	1. The lesser amplitude noise is removed initially using logmmsse based noise reduction technique.
	2. The first step will generate only the audio of the speakers and will remove background noise from the input audio.
	3. Speaker Diarization is applied to the resulting audio from the second step. This segments the audio based on the amplitude of the 		   speaker.
	4. All the audio of the speaker who's amplitude is greater than half of the mean amplitude (Primary Speaker) of the audio from the third 	    step is retained.
	5. This audio is then passed to ASR for the transcript generation.


* The advantages of this methods are :

	1. The method is completely unsupervised.
	2. It does not require any training/validation set.
	3. Outputs the result in real time. ( ~ 2-3 seconds/audio )
	4. The length of audio does not play any role in timing of result.

****************************** Zip File Description ***************************

1. index.py : 

	This is the driver script to run the program.

2. reference_transcripts :
	
	This directory contains the ground truth transcript file for the provided input after removing the noise (text) of the Secondary speaker/speaker's.

3. model_generated_transcripts :
	
	
	This directory contains the generated transcript file by model for the provided input after removing the noise (text) of the Secondary speaker/speaker's.
	
4. README.txt :
	
	This file contains the description of the packaged files from the zip.

5. WER_Score_Analysis.txt
	 
	This file contains the analysed WER score for the given data.



****************************** Tech Stack Used ********************************

1. Programming Language : Python 3.6
2. IDE : Pycharm
3. API Platform Used :  liv.ai


****************************** How to Run  *****************************

* Command to run the program :
	
	python3 index.py

* Flow of the Execution :

The following details has to be considered to run the program :

	1. The model will take following input from the user on Prompting 'Enter the Directory Path for input audio files :' :
		
		This referes to the directory path for input audio files.
		eg : If 1.mp3,2.mp3....n.mp3 is in /home/audio folder, the path will be /home/audio

	2. The model will take following input from the user on Prompting 'Enter the Directory where you wish to save filtered Primary Speaker 		   audio files :'
		
		This referes to the directory path where the output audio of primary speaker will get saved.
		eg : If user wishes to save the output audio files in /home/speaker folder, the path will be /home/speaker


	3. The model will take following input from the user on Prompting 'Enter the Directory where you wish to save text Transcript of output 	   files : '
		
		This referes to the directory path where the output transcript file generated by model will be saved.
		eg : If user wishes to save the output transcript files in /home/transcript folder, the path will be /home/transcript

	
	4. All the files from the source folder will be processed and the output will get listed in respective output directories.

******************************************************************************************************************************************************

	
